读
df = pd.read_csv(url, sep='...', index_col='作为index的列名'/['从多个列','形成分层索引'])
读取之前先观察数据特征，然后决定使用什么参数
path	        指示文件系统位置、URL 或类似文件的对象的字符串。
sep或delimiter	用于拆分每行中的字段的字符序列或正则表达式。
header	        用作列名的行号;默认为 0（第一行），但没有标题行，则应为 <>。None
index_col	    用作结果中行索引的列号或名称;可以是单个名称/编号，也可以是分层索引的列表。
names	        结果的列名列表。
skiprows	    要忽略的文件开头的行数或要跳过的行号列表（从 0 开始）。
na_values	    要替换为 NA 的值序列。除非传递，否则它们将添加到默认列表中。keep_default_na=False
keep_default_na	是否使用默认的NA值列表（默认）。True
comment	        用于将注释从行尾拆分的字符。
parse_dates	    尝试将数据解析为 ; 默认情况下。如果 ，将尝试解析所有列。否则，可以指定要分析的列号或名称的列表。
                如果列表的元素是元组或列表，则将多个列组合在一起并解析为日期（例如，如果日期/时间拆分为两列）。datetimeFalseTrue
keep_date_col	如果连接列以解析日期，请保留连接列; 默认情况下。False
converters	    包含列号或名称映射到函数的字典（例如，将函数应用于列中的所有值）。{"foo": f}f"foo"
dayfirst	    解析可能不明确的日期时，应视为国际格式（例如，7 年 6 月 2012 日至 7 年 2012 月 <> 日>）; 默认情况下。False
date_parser	    用于分析日期的函数。
nrows	        要从文件开头读取的行数（不包括标题）。
iterator	    返回一个对象以逐段读取文件。此对象也可以与语句一起使用。TextFileReaderwith
chunksize	    对于迭代，文件块的大小。
skip_footer	    文件末尾要忽略的行数。
verbose	        打印各种解析信息，例如在文件转换的每个阶段花费的时间和内存使用信息。
encoding	    文本编码（例如，对于 UTF-8 编码的文本）。默认为 if 。"utf-8"utf-8"None
squeeze	        如果解析的数据仅包含一列，则返回 Series。
thousands	    数千个分隔符（例如，或）;默认值为 。",""."None
decimal	        数字中的小数点分隔符（例如，或）;默认值为 。"."",""."
engine	        要使用的CSV解析和转换引擎;可以是 、 或 之一。默认值为 ，尽管较新的引擎可以更快地解析某些文件。
                引擎速度较慢，但支持其他引擎没有的某些功能。"c""python""pyarrow""c""pyarrow""python"

pd.options.display.max_rows = 10

分段读取大型文件
chunker = pd.read_csv("examples/ex6.csv", chunksize=1000) 读取1000行
chunker允许您根据 迭代文件的各个部分
tot = pd.Series([], dtype='int64')
for piece in chunker:
    tot = tot.add(piece["key"].value_counts(), fill_value=0)

tot = tot.sort_values(ascending=False)

处理错误数据
import csv

delimiter	    用于分隔字段的单字符字符串;默认为 。","
lineterminator	用于写入的行终止符;默认为 。读者忽略这一点并识别跨平台行终止符。"\r\n"
quotechar	    为带有特殊字符的字段（如分隔符）引用字符;默认值为 。'"'
quoting	        引用惯例。选项包括（引用所有字段）、（仅具有特殊字符（如分隔符）的字段）和（无引号）。有关完整详细信息，
                请参阅 Python 的文档。默认值为 。csv.QUOTE_ALLcsv.QUOTE_MINIMALcsv.QUOTE_NONNUMERICcsv.QUOTE_NONEQUOTE_MINIMAL
skipinitialspace忽略每个分隔符后的空格;默认值为 。False
doublequote	    如何处理字段中的引用字符;如果为 ，则加倍（有关完整的详细信息和行为，请参阅联机文档）。True
escapechar	    用于转义分隔符的字符串（如果设置为 ）;默认情况下禁用。quotingcsv.QUOTE_NONE


-------------------------------------------------------------------------------------------------------------
df.T
对表进行转置
请注意，如果列不都具有相同的数据类型，则转置将丢弃列数据类型，
因此转置然后转置回来可能会丢失以前的类型信息。在这种情况下，列成为纯 Python 对象的数组。

判断索引标签是否唯一
df.index.is_unique

df.shape
表格形状

df.columns
列名

df.dtype
数据类型名

df.info()
基本信息

df.head()/tail()/info
首/尾/首尾 信息

df.index
表格index
可以通过bool列表取符合要求的index，来对符合要求的行整体操作

army.set_index('origin', inplace=True)
将某一列设为索引
inplace = True 改变原数据

df.groupby()

必须跟着数据处理函数，还要注意对各列的处理方式，及是否要新增列以使数据有意义
依某列成组，后面常接.agg()/transform()/apply分别处理各组
In[]:
price = chipo.groupby('item_name').agg({'price_per_item':'max'})
price.head()
Out[]:
                      price_per_item
item_name
6 Pack Soft Drink                6.49
Barbacoa Bowl                   11.75
Barbacoa Burrito                11.75
Barbacoa Crispy Tacos           11.75
Barbacoa Salad Bowl             11.89
price.take('Salad')

df.groupby(by=[...])依据多列分组
df = df.reset_index()注意常常用此转换索引为习惯形式

pipe
将函数应用于整个GroupBy对象,而不是每个组。

aggregate
对GroupBy对象应用聚合函数。

transform
对GroupBy对象逐列应用函数。

Series.apply
将一个函数应用于一个系列。

DataFrame.apply
对DataFrame的每一行或每一列应用一个函数


Series.map(arg, na_action=None)
根据输入映射或函数映射系列的值。
arg接收字典或函数

Series.apply
用于在系列上应用更复杂的函数。
不会改变原始数据

DataFrame.apply
按行/列应用一个函数。(默认按列）

DataFrame.applymap
在DataFrame的每一个元素上应用一个函数元素。

Series.agg
只执行聚合型操作。
drinks.groupby('continent').spirit_servings.agg(['mean', 'min', 'max'])
对spirit_servings同时执行三种操作

g = data.groupby(['occupation','gender']).gender.count()
occupation     gender     (这里没有overlapping index names)
administrator  F          36
               M          43
artist         F          13
               M          15

gender_ocup = users.groupby(['occupation', 'gender']).agg({'gender': 'count'})

                        gender（用agg才有）
occupation    gender
administrator F           36
              M           43
artist        F           13
              M           15

***使已经被分组到行作为索引的值重新作为一列出现（画图时有用）
gender_ocup['occupation'] = gender_ocup.index.get_level_values(0)

                        gender（用agg才有）  occupation
occupation    gender
administrator F           36                administrator
              M           43                administrator
artist        F           13                artist
              M           15                artist


对指定列使用
gender_ocup = users.groupby(['occupation', 'gender']).agg({'gender': 'count'})

对所有列使用
occup_count = users.groupby(['occupation']).agg('count')

Series.transform
只执行转换类型操作。

unstack( level=- 1 , fill_value=None )
枢轴一个级别的(必然是分级的)索引标签。
Returns a DataFrame having a new level of column labels whose
inner-most level consists of the pivoted index labels


users.列.nunique()
# 等同于users.列.value_counts().count()
计算指定轴上的独特元素的数量。
返回带有不同元素数量的系列。可以忽略NaN值。

value_counts()
df.列名.value_counts()
returns the count of unique elements
对所选列的不同对象进行计数
data.occupation.value_counts()
Out[11]:
occupation
student          196
other            105
educator          95
administrator     79
engineer          67
programmer        66

对整个表格使用该方法，并将NaN转为0(对列的每一个元素)
df.apply(pd.value_counts).fillna(0)
结果中的行标签是所有列中出现的不同值。这些值是每列中这些值的相应计数
Qu1  Qu2  Qu3
1  1.0  1.0  1.0
2  0.0  2.0  1.0
3  2.0  2.0  0.0
4  2.0  0.0  2.0
5  0.0  0.0  1.0

df.value_counts() （将整行视为一个元素计算这样的行出现的次数）
计算计数时将数据帧的每一行视为元组，以确定每个不同行的出现次数
data:
a  b
0  1  0
1  1  0
2  1  1
3  2  0
4  2  0

out:
a  b
1  0    2
2  0    2
1  1    1

pandas.unique(values)
返回基于哈希表的唯一值
Index.unique(level=None)
返回索引中的唯一值
Series.unique()
返回Series对象的唯一值

get_indexer
将数组中每个值的整数索引计算到另一个非重复值数组中
有助于数据对齐和联接类型操作
>>> index = pd.Index(['c', 'a', 'b'])
>>> index.get_indexer(['a', 'b', 'x'])
array([ 1,  2, -1])

DataFrame.isin(values)
DataFrame中的每个元素是否包含在数值中
 df.isin([0, 2])
        num_legs  num_wings
falcon      True       True
dog        False       True

Series.isin(values)
df[df.列名.isin( 列表/Index() )]
系列中的元素是否包含在values
>>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama',
...                'hippo'], name='animal')
>>> s.isin(['cow', 'lama'])
0     True
1     True
2     True
3    False
4     True
5    False

索引

df[column]	从数据帧中选择单列或列序列;特殊情况便利性：布尔数组（筛选器行）、切片（切片行）或布尔数据帧（基于某些条件设置值）

取两列元素
army[['veterans','deaths']]
取行
army[army.origin=='veterans']
利用索引值
df.['b']
利用切片
df.[2:4]

取1到4列元素
hdf[hdf.columns[:4]]

iloc`
纯粹基于整数位置的索引,按位置选择
army.iloc[0]取行
army.iloc[:,1:4]取列
army.iloc[3:8,3:7]取行列

df.iloc[rows]	按整数位置从数据帧中选择单行或行子集
df.iloc[:, cols]	按整数位置选择单列或列子集
df.iloc[rows, cols]	按整数位置选择行和列
df.at[row, col]	按行和列标签选择单个标量值
df.iat[row, col]	按行和列位置（整数）选择单个标量值
reindex方法	按标签选择行或列

df.loc[rows]	按标签从数据帧中选择单行或行子集
df.loc[:, cols]	按标签选择单列或列子集
df.loc[rows, cols]	按标签选择行和列
loc
通过标签或布尔数组访问一组行和列
army.loc[['Maine', 'Alaska'],['deaths', 'size' ,'deserters' ]]
army.loc[army.deaths>50] 取满足该条件的所有行列，等价于army[army.deaths>50]
army.loc[['Arizona','California']]取行
army.loc[:,'deaths'] 取列
army.loc[['Arizona','California'],'deaths']取行列

多层次索引
hdf.loc[(['AA', 'DL', 'US'], ['ORD', 'DSM']), ['dep_time', 'dep_delay']]
        （这里是不同的行索引对应的列）
hdf.loc[pd.IndexSlice[:, ['ORD', 'DSM']], ['dep_time', 'dep_delay']]

取单行单列结合使用
army.loc[:,'deaths'].iloc[-3]
army.loc['Arizona'].iloc[2]


对DataFrame的切片进行更改赋值时使用，否则会报错
customers.loc[:,'Country'] = customers.index.get_level_values(1)
或
customers = customers.copy()
customers['Country'] = customers.index.get_level_values(1)]

利用掩码
mask = wine.alcohol.notnull()
wine.alcohol[mask]

bool下标取法 要求形状相同
chipo:
 #   Column              Non-Null Count  Dtype
---  ------              --------------  -----
 0   order_id            4622 non-null   int64
 1   quantity            4622 non-null   int64
 2   item_name           4622 non-null   object
 3   choice_description  3376 non-null   object
 4   item_price          4622 non-null   object

to get 'Salad' which is in the Column 'item_name':
chipo[chipo.item_name == 'Salad']

数据清理和准备-------------------------------------------------------------------------------------------------------------

缺失值：

判断是否为NaN
df.isna()
df.notna


可利用np.nan来进行缺失值 NaN 赋值

NaN + 数字 = NaN

删除缺失的数值。
dfd.dropna（axis=0，how(删除方式)=_NoDefault.no_default，
thresh（最多保留的NaN值）=_NoDefault.no_default，subset=None， inplace =False）

使用指定方法填写NA/NAN值
df.fillna( value=None , method=None , axis=None ,
                inplace=False , limit=None , downcast=None )
使用字典调用时，可以对每列使用不同的填充值：
df.fillna({1: 0.5, 2: 0})
参数：
value	用于填充缺失值的标量值或类似字典的对象
method	插值法：（向后填充）或（向前填充）中的一种;默认值为"bfill""ffill"None
axis	要填充的轴 （ 或 ）;默认值为"index""columns"axis="index"
limit	对于向前和向后填充，要填充的最大连续周期数

数据转换

检查行是否重复
df.duplicated()
删除重复行
df.drop_duplicates(subset(默认作用于所有列，也可以通过列表指定)=["k1","k2"])

使用函数或映射转换数据
meat_to_animal = {
  "bacon": "pig",
  "pulled pork": "pig",
  "pastrami": "cow",
  "corned beef": "cow",
  "honey ham": "pig",
  "nova lox": "salmon"
}
data["animal"] = data["food"].map(meat_to_animal)
或
def get_animal(x):
        return meat_to_animal[x]
data["food"].map(get_animal)

替换值

df.replace(被替换的值/[多个值], 替换后的值/[对应多个替换])
df.replace({字典})

数据处理-----------------------------------------------------------------

count	非NaN数
describe	计算汇总统计数据集
min, max	计算最小值和最大值
argmin, argmax	分别计算获得最小值或最大值的索引位置（整数）;在数据帧对象上不可用
idxmin, idxmax	分别计算获取最小值或最大值的索引标签
quantile	计算范围从 0 到 1 的样本分位数（默认值：0.5）
sum	值的总和
mean	值的平均值
median	值的算术中位数（50% 分位数(quantile)）
mad	平均值与平均值的绝对偏差
prod	所有值的产品
var	值的样本方差
std	值的样本标准偏差
skew	值的样本偏度（第三个矩(moment)）
kurt	值的样本峰度（第四矩(moment)）
cumsum	值的累积总和
cummin, cummax	值的累积最小值或最大值，分别
cumprod	值的累积乘积
diff	计算第一个算术差值（对时序有用）
pct_change	计算百分比变化

相关性和协方差

序列方法计算两个序列中重叠的、非 NA 的按索引对齐的值的相关性 corr / 协方差 cov
单独计算两个属性间的相关性
returns["MSFT"].corr(returns["IBM"])
Out[283]:0.49976361144151166
计算相关性矩阵
returns.corr()


加强条件筛选
chipo[(chipo.item_name=='Canned Soda') & (chipo.quantity > 1)]
online_rt = online_rt[online_rt.Quantity > 0]

改变数据类型
chipo.item_price = chipo.item_price.apply(lambda x:float(x[1:-1]))
chipo.item_price = pd.to_numeric(chipo.item_price.str.slice(1))
chipo.item_price.str.slice(1) -> 对￥2.31进行切片

# 取所有7月数据
rqr['销售日期']=rqr['销售日期'].astype(str) ***astype
rdp[rdp['销售日期'].str.startswith('2020-07')]


df/列.describe( percentiles=None , include=None ,
                exclude=None , datetime_is_numeric=False )
描述性统计分析，返回一个包含统计结果的 DataFrame。
统计结果包括计数（count）、平均值（mean）、标准差（std）、
最小值（min）、四分位数（25%、50%、75%）和最大值（max）。

合并数据
pd.concat
axis = 0按列增加行数
axis = 1按行增加列数
（对列按axis=0合并，新类形还是列，用 df = s.to_frame()）


pd.merge
按行索引增加列数

DataFrame.join


DataFrame.rename( mapper=None , * , index=None , columns=None , axis=None ,
                copy=None , inplace=False , level=None , errors='ignore' )
改变轴的标签（列/行标签）
使用映射：
df.rename(columns = {原名: 新名, 1: 'bathrs', 2: 'price_sqr_meter'}, inplace=True)
更改类型：
>>> df.index
RangeIndex(start=0, stop=3, step=1)
>>> df.rename(index=str).index
Index(['0', '1', '2'], dtype='object')

设置索引：
DataFrame.reset_index( level=None , drop=False , inplace =False , col_level=0 ,
            col_fill='' , allow_duplicates =_NoDefault.no_default , names=None )
重置指数,或其中的一个级别。
重置DataFrame的索引,并使用默认的索引。如果DataFrame有一个MultiIndex,本方法可以删除一个或多个级别
drop=True: 原索引不会成为新列

df.reindex()
labels	用作索引的新序列。可以是索引实例或任何其他类似序列的 Python 数据结构。
        索引将完全按原样使用，无需任何复制。
index	使用传递的序列作为新的索引标签。
columns	使用传递的序列作为新的列标签。
axis	要重新编制索引的轴，无论是"index"（行）还是 "columns"。reindex(index=new_labels)reindex(columns=new_labels)
method	插值（填充）法; 向前填充"ffill"/向后填充"bfill"。
fill_value	通过重新编制索引引入缺失数据时要使用的替换值。如果希望缺少的标签在结果中具有 null 值，请使用（默认行为）。fill_value="missing"
limit	正向填充或回填时，要填充的最大尺寸间隙（以元素数为单位）。
tolerance	正向填充或回填时，要填充的最大尺寸间隙（以绝对数字距离表示）以查找不精确匹配。
level	在多索引级别上匹配简单索引;否则，请选择子集。
copy	如果 ，即使新索引等效于旧索引，也始终复制基础数据;如果为 ，则当索引等效时，不要复制数据。TrueFalse

删除

简单删除列：
del df['name']

利用函数删除行/列
DataFrame.drop（标签=None，axis=0，index=None，columns=None，
                level=None，inplace=False，errors='raise'）
从行或列中删除指定的标签。
通过指定标签名称和相应的轴,或直接指定索引或列名来移除行或列。当使用多索引时,可以通过指定级别来删除不同级别的标签。

删除指定行
data.drop(index=["Colorado", "Ohio"])

删除指定列
data.drop(columns=["two"])

删除'Total Equity'值为0的行
idx = monthly.index[monthly['Total Equity']==0]
monthly.drop(idx,inplace=True)

删除指定的列
wine = wine.drop(wine.columns[[0,3,6,8,11,12,13]], axis = 1)

截断值：
df.loc[df['...']<0] = 0

更改列的顺序
df = df[['name', 'type', 'hp', 'evolution','pokedex']]


排序
chipo.item_name.sort_values()
chipo.sort_values(by = "item_name")
pandas.DataFrame.sort_values
默认返回一个排序后的视图（共享相同的底层数据。）,而不是在原始 DataFrame 上进行排序。
将 inplace 参数设置为 True，排序操作将直接在原始 DataFrame 上进行，而不会创建一个新的 DataFrame。

discipline.sort_values(['Red Cards', 'Yellow Cards'], ascending = False)
ascending True升序/False降序

在 Pandas 中，& 运算符主要用于对 DataFrame 或 Series
中的元素进行按位与运算，可以用于对特定列或数据框中的所有列进行操作。


遍历行：
for index, row in df.iterrows():
    f(index, row)

取部分行
discipline = euro12[['Team', 'Yellow Cards', 'Red Cards']]

保留小数位
round(float, 保留位数)

DataFrame.max( axis=_NoDefault.no_default , skipna=True , level=None ,
                numeric_only=None , **kwargs )
返回请求轴上的最大值。
如果您想要最大的索引，请使用 idxmax 。这等效于 numpy.ndarray 方法 argmax 。

names[names.Count == names.Count.max()]
idxmax( axis=0 , skipna=True , numeric_only=_NoDefault.no_default )
返回最大值在请求轴上首次出现的索引
crime.idxmax(0)
Out[34]:
（轴）                  （索引）
Population           2014-01-01
Violent              1992-01-01
Property             1991-01-01
Murder               1991-01-01
Forcible_Rape        1992-01-01

DataFrame.cummax( axis=None , skipna=True , *args , **kwargs )
返回DataFrame或Series轴的累计最大值

平均值在Count中出现的地方
names[names.Count == names.Count.median()]

缺失值在每列出现的次数
data.isnull().sum()
RPT    6
VAL    3
ROS    2
KIL    5
dtype: int64
未缺失值的个数
data.notnull().sum()
RPT    6568
VAL    6571
ROS    6572
KIL    6569
dtype: int64


pandas.cut( x , bins , right=True , labels=None , retbins=False , precision=3 ,
include_lowest=False , duplicates='raise' , ordered=True )
Bin值成离散区间，当单个数据太过分散不易观察时适用
Usecut当您需要对数据值进行分割和排序时,可以使用这个函数。这个函数对于从连续变量到分类变量也很有用。
例如,当你需要对数据值进行分割和分类时,这个函数也很有用。cut可以将年龄转换为年龄范围的组。
支持分拣成等量的分仓,或预先指定的分仓数组。

# 划分bucket使数据特征更加清晰
price_start = 0
price_end = 50
price_interval = 1

#Creating the buckets to collect the data accordingly
buckets = np.arange(price_start,price_end,price_interval)

#Select the data and sum
revenue_per_price = online_rt.groupby(pd.cut(online_rt.UnitPrice, buckets)).Revenue.sum()

#作图
revenue_per_price.plot()

#Place labels
plt.xlabel('Unit Price (in buckets of '+str(price_interval)+')')
plt.ylabel('Revenue')

#Even though the data is bucketed in intervals of 1,
#I'll plot ticks a little bit further apart from each other to avoid cluttering.
#设置坐标轴刻度
plt.xticks(np.arange(price_start,price_end,3),
           np.arange(price_start,price_end,3))
plt.yticks([0, 500000, 1000000, 1500000, 2000000, 2500000],
           ['0', '$0.5M', '$1M', '$1.5M', '$2M', '$2.5M'])
plt.show()

#Looks like a major chunk of our revenue comes from items worth $0-$3!


字符串操作----------------------------------------------------------------------------
pandas.Series.str常用于列对象上，大多接受正则
支持功能：

大小写lower/upper/title/capitalize/swapcase

cat
将系列/索引中的字符串用给定的分隔符连接起来
join(sep)
用传递的定界符连接作为系列/索引中元素的列表

测试系列或索引的字符串中是否包含模式或regex (字符序列或正则表达式)contains
(NaN处理，'house|dog'多个参数， 大小写敏感)
match
类似,但更严格,依靠re.match而不是re.search。
Series.str.startswith
测试每个字符串元素的开始是否与模式匹配。
Series.str.endswith
与startwith相同,但测试字符串的末端
find( sub , start=0 , end=None )
返回系列/索引中每个字符串的最低索引
findall(pat, flags=0)
查找系列/索引中所有模式或正则表达式的出现
fullmatch( pat , case=True , flags=0 , na=None )
判断每个字符串是否完全匹配正则表达式

count(pat, flags=0)
计数

decode(encoding, errors='strict')
使用指定的编码对系列/索引中的字符串进行解码

encode(encoding, errors='strict')
使用指定的编码对系列/索引中的字符串进行编码

get(i)
get('name')在指定的位置或以指定的键从每个组件中提取元素。
get(1)从系列/索引中的每个元素的列表、图元、dict或字符串中提取元素

isalnum()
检查每个字符串中的所有字符是否为字母数字
Series.str.isalpha
检查是否所有的字符都是按字母顺序排列的。
Series.str.isnumeric
检查所有字符是否为数字。
Series.str.isalnum
检查所有字符是否为字母数字。
Series.str.isdigit
检查是否所有字符都是数字。
Series.str.isdecimal
检查所有字符是否为十进制。
Series.str.isspace
检查所有的字符是否都是空白。
Series.str.islower
检查所有字符是否为小写。
Series.str.isupper
检查所有字符是否为大写。
Series.str.istitle
检查是否所有字符都是标题大小写。

len()
计算系列/索引中每个元素的长度

ljust( width , fillchar=' ' )
在Series/Index中,在字符串的右侧进行填充


Series.str.strip
删除系列/索引中的前导字符和尾部字符。
s = pd.Series(['1. Ant.  ', '2. Bee!\n', '3. Cat?\t', np.nan, 10, True])
s.str.strip('123.!? \n\t')
Series.str.lstrip
删除系列/索引中的前导字符。
Series.str.rstrip
删除系列/索引中的尾部字符

pad( width , side='left' , fillchar=' ' )
在系列/索引中垫起字符串,宽度不限
 s.str.pad(width=10, side='both', fillchar='-')
0    -caribou--
1    --tiger---

repeat(repeats)
复制系列或索引中的每个字符串

replace( pat , repl , n=- 1 , case=None , flags=0 , regex=None )
替换系列/索引中出现的每个模式/regex

split( pat=None , n=- 1 , expand=False , * , regex=None )
用给定的分隔符/定界符分割字符串

slice( start=None , stop=None , step=None )
从 "系列 "或 "索引 "中的每个元素中切出子串
slice_replace( start=None , stop=None , repl=None )
用另一个值替换一个字符串的位置片断

日期操作----------------------------------------------------------------------------------
import pandas as pd
import datetime
python自带的日期类型到pandas中是object，需要转换pd.to_datetime()



转换为日期类型

将索引转化为日期类型
df.index = pd.to_datetime(df.index)

crime.Year = pd.to_datetime(crime.Year, format='%Y')
(若不加%Y则全都会变成1970-01-01 00:00:00.000001960)
Year  Population     Total  ...  Burglary  Larceny_Theft  Vehicle_Theft
0  1960-01-01   179323175   3384200  ...    912100        1855400         328200
1  1961-01-01   182992000   3488000  ...    949600        1913000         336000
2  1962-01-01   185771000   3752200  ...    994300        2089600         366800
...



划分时间，随后对取样的每相邻两个间的数据进行聚合操作
DataFrame.resample( rule , axis=0 , closed=None , label=None , convention='start' ,
kind=None , loffset=None , base=None , on=None , level=None , origin='start_day' ,
offset=None , group_keys=_NoDefault.no_default )
重新取样时间序列数据。
用于时间序列的频率转换和重新取样的方便方法。该对象必须有一个类似于数据时间的索引
(DatetimeIndex,PeriodIndex, orTimedeltaIndex)，
或者调用者必须将类似日期时间的系列/索引的标签传递给 on / level 关键字参数。
示例
# Uses resample to sum each decade
crimes = crime.resample('10AS').sum()
# Uses resample to get the max value only for the "Population" column
population = crime['Population'].resample('10AS').max()
# Updating the "Population" column
crimes['Population'] = population
#以下两者作用相同
data.resample('W').mean()/data.groupby(data.index.to_period('W')).mean()


以日期为索引时，取每年第一个月求其每一列的平均值
data.loc[data.index.month == 1].mean()

时间段：
DataFrame.to_period( freq=None , axis=0 , copy=True )
将DataFrame从DatetimeIndex转换为PeriodIndex。
将DataFrame从DatetimeIndex转换为具有所需频率的PeriodIndex(如果没有通过,则从索引中推断)
'https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases'
data.groupby(data.index.to_period('Y')).mean()


-----------------------------------------------------------------------------------
# 显示设置
import pandas as pd
#显示所有列
pd.set_option('display.max_columns', None)
#显示所有行
pd.set_option('display.max_rows', None)

#显示1000列
pd.set_option('display.max_columns', 1000)
#显示1000行
pd.set_option('display.max_rows', 1000)
#设置每一列中值的最大显示宽度
pd.set_option('display.max_colwidth',1000)
#设置浮点数的最多显示精度
pd.get_option('display.precision')  # 6
pd.Series(data = [1.23456789])
pd.set_option('display.precision',4)
pd.Series(data = [1.23456789236546])